{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57432854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import random\n",
    "from scipy.stats import rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e576a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Questions :\n",
    "Est-ce que le E dans la formule de perte est une esperance ou une entropie ? \n",
    "A la fin on fait \n",
    "Chaque pacient as X images par dossier est-ce que on considere ce dossier on doit \n",
    "le considerer comme un ensamble d'images et que notre bute final est de debruiter l'ensamble d'images oubien\n",
    "on les considere comme des images separer ? (pour le moment je les considere comme des images separer)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45eff2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2842, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecc8557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_numpy(noised,denoised):\n",
    "    data = []\n",
    "    for i in range(denoised.shape[0]):\n",
    "        pair = np.stack((noised[i],denoised[i]),axis=0)\n",
    "        data.append(pair)\n",
    "        \n",
    "    data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1882d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(noised, denoised):\n",
    "    assert noised.shape == denoised.shape, \"Input tensors must have the same shape.\"\n",
    "    data = torch.stack((noised, denoised), dim=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0eab47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(data):\n",
    "    assert data.shape[1] == 2, \"Input tensor must have as second dim = 2 shape[1]\"\n",
    "    noised, denoised = torch.split(data, 1,dim=1)\n",
    "    noised = noised.squeeze(1)\n",
    "    denoised = denoised.squeeze(1)\n",
    "    return noised, denoised "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704aefd2",
   "metadata": {},
   "source": [
    "General parameters of models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a953ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9621cd",
   "metadata": {},
   "source": [
    "## Generator :\n",
    "The generator takes denoised images and returnes noised images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "670180f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generateur\n",
    "#nb_feat : c'est le nombre de features que notre modele va gere \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nb_feat):\n",
    "        super(Generator, self).__init__()\n",
    "        self.nb_feat = nb_feat\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(nb_feat,100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100,nb_feat),\n",
    "            nn.Tanh()\n",
    "    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e393c1",
   "metadata": {},
   "source": [
    "## Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2fe014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser(nn.Module):\n",
    "    def __init__(self, nb_feat):\n",
    "        super(Denoiser, self).__init__()\n",
    "        self.nb_feat = nb_feat\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(nb_feat,100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100,nb_feat),\n",
    "            nn.Tanh()\n",
    "    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c570e63",
   "metadata": {},
   "source": [
    "## Discriminator \n",
    "Takes two images and has to say if this pair of images is a real one or if it's fake => it takes the stack of the 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d81f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,nb_feat):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(nb_feat,100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100,nb_feat),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f38ed45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(384)\n",
    "denoiser = Denoiser(384)\n",
    "discriminator = Discriminator(384)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be231bc9",
   "metadata": {},
   "source": [
    "The loss function follows the entropie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ee7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        # Example: Custom loss combining standard GAN loss with an additional term\n",
    "        if logits.shape != labels.shape:\n",
    "            raise ValueError(\"Input and labels must have the same shape\")\n",
    "            \n",
    "        sigmoid_logits = torch.sigmoid(logits) \n",
    "        loss = ( labels * torch.log(sigmoid_logits + 1e-12) - \n",
    "                  (1 - labels) * torch.log(1 - sigmoid_logits + 1e-12))\n",
    "\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff4350",
   "metadata": {},
   "source": [
    "Now that we have our loss function we want to initialise the optimizer of our models, that we are going to add in the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27693475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "#optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bd009",
   "metadata": {},
   "source": [
    "Then we have the training loop : \\\n",
    "We know that our dataloader gives us the truth so all the labels are set to true/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb62469",
   "metadata": {},
   "source": [
    "Note : my data contain the denoised image and the the noised one stack behind. Also we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15240fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to call the model we have to :\n",
    "- get the data reformat \n",
    "- create the models\n",
    "- \n",
    "Note the  dataloaders with the noised,denoised and labels \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3c2d6",
   "metadata": {},
   "source": [
    "Note : \\\n",
    "with the use of `.detach()` you ensure that the gradients from the discriminator do not flow back to the generator during the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20962472",
   "metadata": {},
   "source": [
    "We get our transformed data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed8e8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruitage_racien(image,b = 0,loc=0,scale=1):\n",
    "    noise = rice.rvs(b, loc=loc, scale=scale, size=image.shape)\n",
    "    noisy_image = np.clip(image+noise, 0, 255)\n",
    "    \n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7ebb5",
   "metadata": {},
   "source": [
    "For my data I want to change the data I want to extract them all and noised them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2124407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nii_data(source_directory ,typeOfdata = '2_t2_tse_sag'):\n",
    "    all_images = []\n",
    "    #we iterate in the main directory\n",
    "    for name in os.listdir(source_directory):\n",
    "        #iteration for each patient\n",
    "        inner_directory = source_directory+\"/\"+name\n",
    "        for filename in os.listdir(inner_directory):\n",
    "            #extract only the type we want t1,t2 \n",
    "            if typeOfdata in filename :\n",
    "                print(filename)\n",
    "                all_images.append(np.array((nib.load(inner_directory+\"/\"+filename)).dataobj))\n",
    "    # Check if the list is not empty         \n",
    "    if all_images:  \n",
    "        concatenated_images = np.concatenate(all_images, axis=0)\n",
    "    else:\n",
    "        concatenated_images = np.array([])\n",
    "    return concatenated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "228eff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_t2_tse_sag_384.nii\n",
      "2_t2_tse_sag_384.nii\n",
      "2_t2_tse_sag_384.nii\n",
      "2_t2_tse_sag_384.nii\n",
      "2_t2_tse_sag_384.nii\n",
      "2_t2_tse_sag_384.nii\n",
      "2_t2_tse_sag_384.nii\n",
      "2_t2_tse_sag_384.nii\n",
      "2_t2_tse_sag_384.nii\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(133, 384, 384)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = extract_nii_data(\"./test\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "248542b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The data that we noise we know that are images\n",
    "\"\"\"\n",
    "def noise_all_data(fnoise,data):\n",
    "    assert data.ndim == 3,\"Wrong type of data\"\n",
    "    noised = []\n",
    "    for im in data: \n",
    "        noised.append(fnoise(im))\n",
    "    noised = np.array(noised)\n",
    "    return noised   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce5ed1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 384, 384)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised = noise_all_data(bruitage_racien,data)\n",
    "noised.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db81cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(np.rot90(noised[0]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60e76b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(np.rot90(data[0]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aabf64fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([133, 2, 384, 384])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoised_torch = torch.tensor(data, dtype=torch.float32) \n",
    "noised_torch = torch.tensor(noised, dtype=torch.float32) \n",
    "\n",
    "data_torch = format_data(noised_torch,denoised_torch)\n",
    "\n",
    "data_torch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27032779",
   "metadata": {},
   "source": [
    "Note the labels will depend on the MODEL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8135976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([10, 384, 384]), noised: torch.Size([10, 384, 384])\n",
      "denoised : torch.Size([3, 384, 384]), noised: torch.Size([3, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "#data_tensor = torch.from_numpy(data).float()/255.0\n",
    "dataset = TensorDataset(denoised_torch, noised_torch)\n",
    "\n",
    "batch_size = 10 \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#veryfication loop\n",
    "for batch_denoised, batch_noised in dataloader:\n",
    "    print(f\"denoised : {batch_denoised.shape}, noised: {batch_noised.shape}\")\n",
    "\n",
    "batch_denoised, batch_noised = next(iter(dataloader))\n",
    "noised_image = batch_noised[0]\n",
    "denoised_image = batch_denoised[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d7bd62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader,num_epochs,g_model,d_model,r_model,nb_feat,alpha,n_critic,loss_fn):\n",
    "    \n",
    "    optimizer_g = torch.optim.Adam(g_model.parameters(), lr=lr)\n",
    "    optimizer_d = torch.optim.Adam(d_model.parameters(), lr=lr)\n",
    "    optimizer_r = torch.optim.Adam(r_model.parameters(), lr=lr)\n",
    "    \n",
    "    g_model = g_model.to(device)\n",
    "    d_model = d_model.to(device)\n",
    "    r_model = r_model.to(device)\n",
    "    \n",
    "    #while loss small engouh\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        print(f\"epoch {e} done\")\n",
    "        for n in range(n_critic):\n",
    "            print(f\"critic {n} done\")\n",
    "            for i, (denoised,noised) in enumerate(data_loader):\n",
    "\n",
    "                #place it on the machine \n",
    "                noised = noised.to(device)\n",
    "                denoised = denoised.to(device)\n",
    "                #def separate to noised and denoised\n",
    "            \n",
    "\n",
    "                #labels\n",
    "                fake_labels = torch.zeros(noised.size(0), 1, device=device)\n",
    "                real_labels = torch.ones(noised.size(0), 1, device=device)\n",
    "\n",
    "\n",
    "                ########################\n",
    "                #Discriminator Training# \n",
    "                ########################\n",
    "\n",
    "                optimizer_d.zero_grad()\n",
    "\n",
    "                #optimize the discriminator by giving the real images\n",
    "\n",
    "                real_data = format_data(noised,denoised)\n",
    "                real_output = d_model(real_data)\n",
    "\n",
    "                #generate fake data (start by giving a random noise)\n",
    "                z = torch.randn(batch_size, nb_feat,nb_feat, device=device)#TODO : fix the size depending of the dataloaders\n",
    "                fake_data_noised = g_model(z)\n",
    "                fake_data_denoised = r_model(noised)\n",
    "                print(fake_data_noised.shape)\n",
    "                print(fake_data_denoised.shape)\n",
    "\n",
    "                #(x_hat,y) : fake real\n",
    "                fake_real = format_data(fake_data_noised,denoised)\n",
    "                fake_real_out = d_model(fake_real)\n",
    "\n",
    "                #(x,y_hat) : real fake\n",
    "                real_fake = format_data(noised,fake_data_denoised)\n",
    "                real_fake_out = d_model(real_fake)\n",
    "\n",
    "                #optimize the discriminator by giving the generated images\n",
    "\n",
    "                d_loss_real = loss_fn(real_output,real_labels)\n",
    "                d_loss_real_fake = loss_fn(real_fake_out,fake_labels)\n",
    "                d_loss_fake_real = loss_fn(fake_real_out,fake_labels)\n",
    "\n",
    "                d_loss = d_loss_real - (alpha)*d_loss_fake_real - (1-alpha)*d_loss_real_fake \n",
    "                print(d_loss)\n",
    "                d_loss.backward()\n",
    "                optimizer_d.step()\n",
    "\n",
    "                # Print losses\n",
    "                if i % 100 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], '\n",
    "                      f'Discriminator Loss: {d_loss.item():.4f}')\n",
    "                    \n",
    "        for i, (denoised,noised) in enumerate(data_loader):\n",
    "\n",
    "            #place it on the machine \n",
    "            noised = noised.to(device)\n",
    "            denoised = denoised.to(device)\n",
    "\n",
    "            #labels\n",
    "            fake_labels = torch.zeros(noised.size(0), 1, device=device)\n",
    "            real_labels = torch.ones(noised.size(0), 1, device=device)\n",
    "\n",
    "\n",
    "            ####################\n",
    "            # Update Generator # \n",
    "            ####################\n",
    "\n",
    "            optimizer_g.zero_grad()\n",
    "\n",
    "\n",
    "            fake_data_denoised = r_model(noised)\n",
    "            #fake_data = fake_data_noised + fake_data_denoised\n",
    "            real_fake = format_data(noised,fake_data_denoised)\n",
    "            real_fake_out = d_model(real_fake)\n",
    "\n",
    "\n",
    "            g_loss =  alpha*loss_fn(real_fake_out,fake_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            ###################\n",
    "            # Update Denoizer # \n",
    "            ###################\n",
    "\n",
    "            optimizer_r.zero_grad()\n",
    "\n",
    "            fake_data_noised = g_model(denoised)\n",
    "            fake_real = format_data(fake_data_noised,denoised)\n",
    "            fake_real_out = d_model(fake_real)\n",
    "\n",
    "            r_loss =(1-alpha)*loss_fn(fake_real_out,fake_labels)\n",
    "            r_loss.backward()\n",
    "            optimizer_r.step()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], '\n",
    "                      f'R : Denoiser Loss: {r_loss.item():.4f}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aae964a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 done\n",
      "critic 0 done\n",
      "torch.Size([10, 384, 384])\n",
      "torch.Size([10, 384, 384])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([10, 1])) must be the same as input size (torch.Size([10, 2, 384, 384]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0002\u001b[39m\n\u001b[1;32m      2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m----> 3\u001b[0m train(dataloader,\u001b[38;5;241m2\u001b[39m,generator,discriminator,denoiser,\u001b[38;5;241m384\u001b[39m,\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m2\u001b[39m,loss_fn)\n",
      "Cell \u001b[0;32mIn[71], line 58\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_loader, num_epochs, g_model, d_model, r_model, nb_feat, alpha, n_critic, loss_fn)\u001b[0m\n\u001b[1;32m     54\u001b[0m real_fake_out \u001b[38;5;241m=\u001b[39m d_model(real_fake)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#optimize the discriminator by giving the generated images\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m loss_fn(real_output,real_labels)\n\u001b[1;32m     59\u001b[0m d_loss_real_fake \u001b[38;5;241m=\u001b[39m loss_fn(real_fake_out,fake_labels)\n\u001b[1;32m     60\u001b[0m d_loss_fake_real \u001b[38;5;241m=\u001b[39m loss_fn(fake_real_out,fake_labels)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:821\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    823\u001b[0m         target,\n\u001b[1;32m    824\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    825\u001b[0m         pos_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_weight,\n\u001b[1;32m    826\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m    827\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:3639\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3636\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3641\u001b[0m     )\n\u001b[1;32m   3643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m   3644\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[1;32m   3645\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([10, 1])) must be the same as input size (torch.Size([10, 2, 384, 384]))"
     ]
    }
   ],
   "source": [
    "lr = 0.0002\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "train(dataloader,2,generator,discriminator,denoiser,384,0.5,2,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c95561",
   "metadata": {},
   "source": [
    "Have to think about the labaling :\n",
    "So easy for the denoiser and for the generator because we only have 1 input.For the discriminator we consider that if at least one is not the real one then the label is fake so 0 otherwise it's 1(true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b39e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
