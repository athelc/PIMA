{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "511031b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import random\n",
    "from scipy.stats import rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f46fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3785, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44d58c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_numpy(noised,denoised):\n",
    "    data = []\n",
    "    for i in range(denoised.shape[0]):\n",
    "        pair = np.stack((noised[i],denoised[i]),axis=0)\n",
    "        data.append(pair)\n",
    "        \n",
    "    data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c20bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(noised, denoised):\n",
    "    assert noised.shape == denoised.shape, \"Input tensors must have the same shape.\"\n",
    "    data = torch.stack((noised, denoised), dim=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bb6dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(data):\n",
    "    assert data.shape[1] == 2, \"Input tensor must have as second dim = 2 shape[1]\"\n",
    "    noised, denoised = torch.split(data, 1,dim=1)\n",
    "    noised = noised.squeeze(1)\n",
    "    denoised = denoised.squeeze(1)\n",
    "    return noised, denoised "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839591dc",
   "metadata": {},
   "source": [
    "General parameters of models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec9bed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dba5a7",
   "metadata": {},
   "source": [
    "## Generator :\n",
    "The generator takes denoised images and returnes noised images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed542971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generateur\n",
    "#nb_feat : c'est le nombre de features que notre modele va gere \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nb_feat):\n",
    "        super(Generator, self).__init__()\n",
    "        self.nb_feat = nb_feat\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(nb_feat,100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100,nb_feat),\n",
    "            nn.Tanh()\n",
    "    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a48fc",
   "metadata": {},
   "source": [
    "## Denoiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4923ef",
   "metadata": {},
   "source": [
    "## Discriminator \n",
    "Takes two images and has to say if this pair of images is a real one or if it's fake => it takes the stack of the 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e09d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,nb_feat):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(nb_feat,100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100,nb_feat),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9cc017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(50)\n",
    "discriminator = Discriminator(50)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9681bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        true_class_probabilities = probabilities[range(logits.size(0)), target]\n",
    "        loss = -torch.log(true_class_probabilities + 1e-10)  # Avoid log(0)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, apha=0.5):\n",
    "        super(CustomBCEWithLogitsLoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        # Ensure input and target are of the same shape\n",
    "        if inputs.shape != target.shape:\n",
    "            raise ValueError(\"Input and target must have the same shape\")\n",
    "\n",
    "        # Compute the sigmoid of the input logits\n",
    "        sigmoid_input = torch.sigmoid(inputs)\n",
    "\n",
    "        # Compute the binary cross-entropy loss\n",
    "        loss = - (target * torch.log(sigmoid_input + 1e-12) + \n",
    "                  (1 - target) * torch.log(1 - sigmoid_input + 1e-12))\n",
    "\n",
    "        # Apply positive weight if provided\n",
    "        if self.pos_weight is not None:\n",
    "            loss = loss * (self.pos_weight * target + (1 - target))\n",
    "\n",
    "        return loss.mean()  # Return the mean loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6475cb",
   "metadata": {},
   "source": [
    "The loss function follows the entropie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLoss(nn.Module):\n",
    "    def __init__(self,alpha = 0.5):\n",
    "        super(FLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        # Example: Custom loss combining standard GAN loss with an additional term\n",
    "        if logits.shape != labels.shape:\n",
    "            raise ValueError(\"Input and labels must have the same shape\")\n",
    "            \n",
    "        sigmoid_logits = torch.sigmoid(logits) \n",
    "        loss = ( labels * torch.log(sigmoid_logits + 1e-12) - \n",
    "                  alpha*(1 - labels) * torch.log(1 - sigmoid_logits + 1e-12))\n",
    "\n",
    "        \n",
    "        # You can add custom terms here if needed\n",
    "        # For example, you could add a regularization term or a penalty\n",
    "        # loss_g += some_custom_penalty\n",
    "        \n",
    "        return loss_d, loss_g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e158ba2",
   "metadata": {},
   "source": [
    "Now that we have our loss function we want to initialise the optimizer of our models, that we are going to add in the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c04ae156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "#optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f770b3cf",
   "metadata": {},
   "source": [
    "Then we have the training loop : \\\n",
    "We know that our dataloader gives us the truth so all the labels are set to true/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8e1b9",
   "metadata": {},
   "source": [
    "Note : my data contain the denoised image and the the noised one stack behind. Also we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88272c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader,num_epochs,g_model,d_model,r_model,nb_feat):#\n",
    "    \n",
    "    optimizer_g = torch.optim.Adam(g_model.parameters(), lr=lr)\n",
    "    optimizer_d = torch.optim.Adam(d_model.parameters(), lr=lr)\n",
    "    optimizer_r = torch.optim.Adam(r_model.parameters(), lr=lr)\n",
    "    \n",
    "    g_model = g_model.to(device)\n",
    "    d_model = d_model.to(device)\n",
    "    r_model = r_model.to(device)\n",
    "    \n",
    "    #while loss small engouh\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        for i, (real_data,_) in enumerate(data_loader):\n",
    "            \n",
    "            #place it on the machine \n",
    "            real_data = real_data.to(device)\n",
    "            #def separate to noised and denoised\n",
    "            noised,denoised = separate_data(real_data)\n",
    "            \n",
    "            #labels\n",
    "            fake_labels = torch.zeros(real_data.size(0), 1, device=device)\n",
    "            valid_labels = torch.ones(real_data.size(0)*3, 1, device=device)\n",
    "            \n",
    "            \n",
    "            ########################\n",
    "            #Discriminator Training# \n",
    "            ########################\n",
    "            \n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            #optimize the discriminator by giving the real images\n",
    "            \n",
    "            outputs = d_model(real_data)\n",
    "            lossD_valid = fLoss(outputs, valid_labels)\n",
    "            lossD_valid.backward()\n",
    "            \n",
    "            #generate fake data (start by giving a random noise)\n",
    "            z = torch.randn(batch_size, nb_feat, device=device)#TODO : fix the size depending of the dataloaders\n",
    "            fake_data_noised = g_model(z)\n",
    "            fake_data_denoised = r_model(noised_data)\n",
    "            \n",
    "            #stack fake_data_ = fake_data_noised + fake_data_denoised (format_data(noised, denoised))\n",
    "            #We have 4 different types of data : \n",
    "            #(x,y):real data with labels 1 -> right/true\n",
    "            \n",
    "            #the labels for all of these will be zero -> wrong/false\n",
    "            #(x_hat,y_hat):fake data\n",
    "            fake_fake = format_data(fake_data_noised,fake_data_denoised)\n",
    "            #(x_hat,y) : fake real\n",
    "            fake_real = format_data(fake_data_noised,denoised)\n",
    "            #(x,y_hat) : real fake\n",
    "            real_fake = format_data(noised,fake_data_denoised)\n",
    "            \n",
    "            seperated_data =[fake_fake,fake_real,real_fake]\n",
    "            all_fake_data = torch.cat(seperated_data, dim=0)\n",
    "            \n",
    "            #optimize the discriminator by giving the generated images\n",
    "            outputs = d_model(all_fake_data)\n",
    "            lossD_f = fLoss(outputs, fake_labels)\n",
    "            lossD_f.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Print losses\n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], '\n",
    "                  f'Discriminator Loss: {lossD_valid.item() + lossD_f.item():.4f}')\n",
    "            \n",
    "        ####################\n",
    "        # Update Generator # \n",
    "        ####################\n",
    "            \n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        #we fix the denoiser and the discriminator\n",
    "        \n",
    "        #fake_data_denoised = r_model(data)\n",
    "        #fake_data = fake_data_noised + fake_data_denoised\n",
    "        #output = d_model(fake_data)\n",
    "        \n",
    "        #loss_g = fLoss(outputs, fake_labels)\n",
    "        #loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "            \n",
    "        ###################\n",
    "        # Update Denoizer # \n",
    "        ###################\n",
    "            \n",
    "        optimizer_r.zero_grad()\n",
    "            \n",
    "        #fake_data_noised = g_model(data)\n",
    "        #fake_data = fake_data_noised + fake_data_denoised\n",
    "        #output = d_model(fake_data)\n",
    "        \n",
    "        #loss_r = fLoss(outputs, fake_labels)\n",
    "        #loss_r.backward()\n",
    "        \n",
    "        optimizer_r.step()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dcea33",
   "metadata": {},
   "source": [
    "Note : \\\n",
    "with the use of `.detach()` you ensure that the gradients from the discriminator do not flow back to the generator during the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f6559",
   "metadata": {},
   "source": [
    "We get our transformed data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c8bc1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruitage_racien(image,b = 0,loc=0,scale=1):\n",
    "    noise = rice.rvs(b, loc=loc, scale=scale, size=image.shape)\n",
    "    noisy_image = np.clip(image+noise, 0, 255)\n",
    "    \n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbd8aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example file\n",
    "filename = \"./IMA_project/PIMA/test/0001/2_t2_tse_sag_384.nii\"\n",
    "\n",
    "data = np.array((nib.load(filename)).dataobj) #in this we have 15 slices\n",
    "labels = torch.ones(data.shape[0])\n",
    "\n",
    "noised = []\n",
    "for im in data: \n",
    "    noised.append(bruitage_racien(im))\n",
    "    \n",
    "noised = np.array(noised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04de4a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 2, 384, 384), torch.Size([15, 2, 384, 384]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_im = format_data_numpy(data,noised)\n",
    "\n",
    "denoised_torch = torch.tensor(data, dtype=torch.float32) \n",
    "noised_torch = torch.tensor(noised, dtype=torch.float32) \n",
    "\n",
    "data_torch = format_data(noised_torch,denoised_torch)\n",
    "\n",
    "all_im.shape,data_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8de82bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "snoi,sden = separate_data(data_torch)\n",
    "if sden.requires_grad:\n",
    "    sden = sden.detach()\n",
    "# Convert to NumPy array\n",
    "t_im = sden.numpy()\n",
    "#plt.imshow(t_im[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb719c",
   "metadata": {},
   "source": [
    "Note the labels will depend on the MODEL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39d72224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([4, 384, 384]), Labels: tensor([0, 1, 0, 0])\n",
      "Batch shape: torch.Size([4, 384, 384]), Labels: tensor([0, 0, 0, 0])\n",
      "Batch shape: torch.Size([4, 384, 384]), Labels: tensor([1, 0, 1, 0])\n",
      "Batch shape: torch.Size([3, 384, 384]), Labels: tensor([1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "data_tensor = torch.from_numpy(data).float()/255.0\n",
    "dataset = TensorDataset(data_tensor, labels)\n",
    "\n",
    "batch_size = 4 \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#veryfication loop\n",
    "for batch_images, batch_labels in dataloader:\n",
    "    print(f\"Batch shape: {batch_images.shape}, Labels: {batch_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e4236f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([384, 384]), Labels: tensor([0, 0, 0, 1])\n",
      "Batch shape: torch.Size([384, 384]), Labels: tensor([0, 1, 0, 0])\n",
      "Batch shape: torch.Size([384, 384]), Labels: tensor([0, 0, 1, 0])\n",
      "Batch shape: torch.Size([384, 384]), Labels: tensor([1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch_images, batch_labels in dataloader:\n",
    "    print(f\"Batch shape: {batch_images.shape[1:]}, Labels: {batch_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aab6d1",
   "metadata": {},
   "source": [
    "Have to think about the labaling :\n",
    "So easy for the denoiser and for the generator because we only have 1 input.For the discriminator we consider that if at least one is not the real one then the label is fake so 0 otherwise it's 1(true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe179adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
